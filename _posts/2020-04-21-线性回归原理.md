### 线性回归原理

​	给定由d个属性描述的示例X = (x1;x2;...;xd),其中xi是X在第i 个属性上的取值，线性模型(linear model)试图学得一个通过属性的线性组合进行预测的函数。即
$$
f(x) = w_1x_1 + w_2x_2 + ...+w_dx_d +b
$$
一般用向量形式写
$$
f(x) = W^TX +b
$$
其中W = (w1;w2;...;wd)。W和b是学得之后，模型就得以确定。

### 损失函数

#### 	RMSE

​		回归问题的典型性能衡量指标是均方根误差,它测量是预测过程中，预测错误的**标准差**。
$$
RMSE(X, f) = \frac{1}{m}\sqrt\sum_{i=1}^n(f(X^i) - y^i)^2
$$

#### 	MAE

即便RMSE通常是回归任务的首选性能衡量指标，但是在某些情况下，其他函数可能会更适合。例如当有很多离群区域时，你可以考虑使用平均绝对误差。
$$
MAE(X, f) = \frac{1}{m}\sum_{i=1}^n|f(X^i) - y^i|
$$

### 线性回归的优化方法

	#### 最小二乘法

​	对公式
$$
MSE = \sum_{i=1}^n(y_i - x_iw)^2
$$
​	求导得
$$
\hat w = (X^TX)^{-1} X^Ty
$$
值得注意的是这个方程只有在逆矩阵存在的时候使用，然而矩阵的逆可能并不存在，因此梯度下降可以解决这个问题。

#### 梯度下降法

梯度下降是一种对损失进行优化的算法，梯度的方向就是函数增大的方向，如果我们对梯度进行取反，就是梯度下降法。
$$
J(\theta) = \frac{1}{2m}\sum_{1}^m(f_\theta(x^i) - y^i)
$$
​	对每一个θ求导：
$$
\frac{\nabla J(\theta)}{\nabla \theta_j} = \frac{1}{m}\sum_{1}^m(f_\theta(x^i) - y^i)x_j^i
$$
迭代公式,对导数取反。η又称之为学习率。
$$
\theta = \theta - \eta.\nabla J(\theta)
$$
学习了太大可能使得损失函数不收敛。太小会使得下降的速度慢，使得学习得过程比较慢。

### 测试代码

#### 	最小二乘法求解

```python
import numpy as np
from matplotlib import pyplot as plt

x = np.random.rand(500, 3)
y = x.dot(np.array([1.2, 2.3, 3.4]))
# 使用最小二乘法求解
class MyLinear():
    
    def __init__(self):
        self.w = None
        
    def fit(self, X, y):
        self.w = np.linalg.inv((X.T.dot(X))).dot(X.T).dot(y)
        
    def pridect(X):
        return X.doct(self.w.T)
    
lin = MyLinear()
lin.fit(x, y)
lin.w
```

结果：array([1.2, 2.3, 3.4])

#### 	梯度下降法求解

```python
# 梯度下降法求解
class Linear_GD():
    
    def __init__(self):
        self.w = None
        
    def fit(self, X, y, eta=0.01, loss=1e-10):
        y = y.reshape(-1, 1)
        d = X.shape[-1]
        self.w = np.zeros((d))
        tol = 1e5
        while tol > loss:
            f_w = X.dot(self.w).reshape(-1,1) 
            theta = self.w -  eta * np.mean((f_w - y)*X, axis=0)
            tol = np.abs(np.sum(X.dot(theta.T) - X.dot(self.w.T)))
            self.w = theta
    def pridect(X):
        return X.dot(self.w.T)
```

结果：array([1.20000023, 2.29999993, 3.39999984])

###  sklearn中的线性回归

```python
from sklearn.linear_model import LinearRegression
LG1 = LinearRegression()
LG1.fit(x, y)
LG1.coef_
```

结果：array([1.2, 2.3, 3.4])

参数说明：

```json
fit_intercept: 默认为True,是否计算该模型的截距。如果使用中心化的数据，可以考虑设置为False,不考虑截距。注意这里是考虑，一般还是要考虑截距
normalize:默认为false. 当fit_intercept设置为false的时候，这个参数会被自动忽略。如果为True,回归器会标准化输入参数：减去平均值，并且除以相应的二范数。当然啦，在这里还是建议将标准化的工作放在训练模型之前。通过设置sklearn.preprocessing.StandardScaler来实现，而在此处设置为false
copy_X:默认为True, 否则X会被改写
n_jobs: int 默认为1. 当-1时默认使用全部CPUs
```

可用的属性：

```json
coef_:训练后的输入端模型系数，如果label有两个，即y值有两列。那么是一个2D的array
intercept_: 截距
```